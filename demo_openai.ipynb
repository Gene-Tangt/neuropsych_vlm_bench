{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42689b21",
   "metadata": {},
   "source": [
    "# Code demo with OpenAI API\n",
    "\n",
    "The current jupyter notebook demonstrates the workflow of the benchmark using OpenAI API as an example. This notebook will also breakdown core components and functions used in this repository, including loading task data, running models, and evaluating responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from loaders import TaskLoader # for loading tasks from the test_specs directory\n",
    "from runner import ModelConfig, OpenAIModelRunner # for setting up OpenAI config and runner\n",
    "from evaluator import Evaluator # for automatically evaluating responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb0ac83",
   "metadata": {},
   "source": [
    "## Load API Keys\n",
    "\n",
    "In order to run the models, we need to first load the API keys. This should be stored in a separate file (e.g. api_keys.json). The api_keys file structure should be as follows:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"open_ai\": \"your_openai_api_key\",\n",
    "    \"anthropic\": \"your_anthropic_api_key\",\n",
    "    \"google\": \"your_google_api_key\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "load-config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 31 tasks\n",
      "Sample tasks:\n",
      "  0: test_specs/low/borb_orientation_meta.json\n",
      "  1: test_specs/low/borb_line_length_comparison_meta.json\n",
      "  2: test_specs/low/borb_size_comparison_meta.json\n"
     ]
    }
   ],
   "source": [
    "# Get API keys\n",
    "with open(\"utils/api_keys.json\", \"r\") as f:\n",
    "    api_keys = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c85388",
   "metadata": {},
   "source": [
    "## Inspect Available Tests\n",
    "\n",
    "The code below shows a sample of tests available in the test_specs directory.\n",
    "In the paper: https://arxiv.org/abs/2504.10786v1. Neuropsychological tests are categorized into which visual processing stage they are designed to tap into- low, mid, or high. The tests in test_specs are categorized into these stages.\n",
    "\n",
    "Each task metadata file e.g., `borb_line_length_comparison_meta.json` contains information about the task such as task groupings, task name, task type, number of stimuli (used to determine prompt format), the prompt, and also trial-by-trial information (e.g., stimuli paths and answer keys). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8cdfaf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 31 tasks\n",
      "Sample tasks:\n",
      "  0: test_specs/low/borb_orientation_meta.json\n",
      "  1: test_specs/low/borb_line_length_comparison_meta.json\n",
      "  2: test_specs/low/borb_size_comparison_meta.json\n"
     ]
    }
   ],
   "source": [
    "# Load task paths\n",
    "with open(\"test_specs/test_list.json\", 'r') as file:\n",
    "    all_task_paths = []\n",
    "    for stage in json.load(file):\n",
    "        all_task_paths.extend(stage['task_paths'])\n",
    "\n",
    "# Print the number of tasks available\n",
    "print(f\"Found {len(all_task_paths)} tasks\")\n",
    "\n",
    "# Print sample tasks\n",
    "print(\"Sample tasks:\")\n",
    "for i, path in enumerate(all_task_paths[:3]): # print first 3 tasks\n",
    "    print(f\"  {i}: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25448df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'low', 'process': 'simple_element_judgements', 'task': 'borb_line_length_comparison', 'task_type': 'same_different', 'num_stim': 'two', 'prompt': \"This is a task. In this task you will tell me if these two lines are the same length or not. Answer with only 'same' or 'different'. Please put your final answer in {}.\", 'trials': [{'trial': 'trial_001', 'images': {'options': ['datasets/low/borb_line_length_comparison/trial_001/A.png', 'datasets/low/borb_line_length_comparison/trial_001/B.png']}, 'answer_key': 'different'}, {'trial': 'trial_002', 'images': {'options': ['datasets/low/borb_line_length_comparison/trial_002/A.png', 'datasets/low/borb_line_length_comparison/trial_002/B.png']}, 'answer_key': 'same'}, {'trial': 'trial_003', 'images': {'options': ['datasets/low/borb_line_length_comparison/trial_003/A.png', 'datasets/low/borb_line_length_comparison/trial_003/B.png']}, 'answer_key': 'same'}, {'trial': 'trial_004', 'images': {'options': ['datasets/low/borb_line_length_comparison/trial_004/A.png', 'datasets/low/borb_line_length_comparison/trial_004/B.png']}, 'answer_key': 'different'}, {'trial': 'trial_005', 'images': {'options': ['datasets/low/borb_line_length_comparison/trial_005/A.png', 'datasets/low/borb_line_length_comparison/trial_005/B.png']}, 'answer_key': 'different'}, {'trial': 'trial_006', 'images': {'options': ['datasets/low/borb_line_length_comparison/trial_006/A.png', 'datasets/low/borb_line_length_comparison/trial_006/B.png']}, 'answer_key': 'same'}, {'trial': 'trial_007', 'images': {'options': ['datasets/low/borb_line_length_comparison/trial_007/A.png', 'datasets/low/borb_line_length_comparison/trial_007/B.png']}, 'answer_key': 'different'}, {'trial': 'trial_008', 'images': {'options': ['datasets/low/borb_line_length_comparison/trial_008/A.png', 'datasets/low/borb_line_length_comparison/trial_008/B.png']}, 'answer_key': 'same'}, {'trial': 'trial_009', 'images': {'options': ['datasets/low/borb_line_length_comparison/trial_009/A.png', 'datasets/low/borb_line_length_comparison/trial_009/B.png']}, 'answer_key': 'different'}, {'trial': 'trial_010', 'images': {'options': ['datasets/low/borb_line_length_comparison/trial_010/A.png', 'datasets/low/borb_line_length_comparison/trial_010/B.png']}, 'answer_key': 'same'}, {'trial': 'trial_011', 'images': {'options': ['datasets/low/borb_line_length_comparison/trial_011/A.png', 'datasets/low/borb_line_length_comparison/trial_011/B.png']}, 'answer_key': 'same'}, {'trial': 'trial_012', 'images': {'options': ['datasets/low/borb_line_length_comparison/trial_012/A.png', 'datasets/low/borb_line_length_comparison/trial_012/B.png']}, 'answer_key': 'different'}, {'trial': 'trial_013', 'images': {'options': ['datasets/low/borb_line_length_comparison/trial_013/A.png', 'datasets/low/borb_line_length_comparison/trial_013/B.png']}, 'answer_key': 'same'}, {'trial': 'trial_014', 'images': {'options': ['datasets/low/borb_line_length_comparison/trial_014/A.png', 'datasets/low/borb_line_length_comparison/trial_014/B.png']}, 'answer_key': 'different'}, {'trial': 'trial_015', 'images': {'options': ['datasets/low/borb_line_length_comparison/trial_015/A.png', 'datasets/low/borb_line_length_comparison/trial_015/B.png']}, 'answer_key': 'different'}, {'trial': 'trial_016', 'images': {'options': ['datasets/low/borb_line_length_comparison/trial_016/A.png', 'datasets/low/borb_line_length_comparison/trial_016/B.png']}, 'answer_key': 'different'}, {'trial': 'trial_017', 'images': {'options': ['datasets/low/borb_line_length_comparison/trial_017/A.png', 'datasets/low/borb_line_length_comparison/trial_017/B.png']}, 'answer_key': 'same'}, {'trial': 'trial_018', 'images': {'options': ['datasets/low/borb_line_length_comparison/trial_018/A.png', 'datasets/low/borb_line_length_comparison/trial_018/B.png']}, 'answer_key': 'different'}, {'trial': 'trial_019', 'images': {'options': ['datasets/low/borb_line_length_comparison/trial_019/A.png', 'datasets/low/borb_line_length_comparison/trial_019/B.png']}, 'answer_key': 'same'}, {'trial': 'trial_020', 'images': {'options': ['datasets/low/borb_line_length_comparison/trial_020/A.png', 'datasets/low/borb_line_length_comparison/trial_020/B.png']}, 'answer_key': 'same'}, {'trial': 'trial_021', 'images': {'options': ['datasets/low/borb_line_length_comparison/trial_021/A.png', 'datasets/low/borb_line_length_comparison/trial_021/B.png']}, 'answer_key': 'different'}, {'trial': 'trial_022', 'images': {'options': ['datasets/low/borb_line_length_comparison/trial_022/A.png', 'datasets/low/borb_line_length_comparison/trial_022/B.png']}, 'answer_key': 'same'}, {'trial': 'trial_023', 'images': {'options': ['datasets/low/borb_line_length_comparison/trial_023/A.png', 'datasets/low/borb_line_length_comparison/trial_023/B.png']}, 'answer_key': 'same'}, {'trial': 'trial_024', 'images': {'options': ['datasets/low/borb_line_length_comparison/trial_024/A.png', 'datasets/low/borb_line_length_comparison/trial_024/B.png']}, 'answer_key': 'different'}, {'trial': 'trial_025', 'images': {'options': ['datasets/low/borb_line_length_comparison/trial_025/A.png', 'datasets/low/borb_line_length_comparison/trial_025/B.png']}, 'answer_key': 'same'}, {'trial': 'trial_026', 'images': {'options': ['datasets/low/borb_line_length_comparison/trial_026/A.png', 'datasets/low/borb_line_length_comparison/trial_026/B.png']}, 'answer_key': 'different'}, {'trial': 'trial_027', 'images': {'options': ['datasets/low/borb_line_length_comparison/trial_027/A.png', 'datasets/low/borb_line_length_comparison/trial_027/B.png']}, 'answer_key': 'different'}, {'trial': 'trial_028', 'images': {'options': ['datasets/low/borb_line_length_comparison/trial_028/A.png', 'datasets/low/borb_line_length_comparison/trial_028/B.png']}, 'answer_key': 'different'}, {'trial': 'trial_029', 'images': {'options': ['datasets/low/borb_line_length_comparison/trial_029/A.png', 'datasets/low/borb_line_length_comparison/trial_029/B.png']}, 'answer_key': 'same'}, {'trial': 'trial_030', 'images': {'options': ['datasets/low/borb_line_length_comparison/trial_030/A.png', 'datasets/low/borb_line_length_comparison/trial_030/B.png']}, 'answer_key': 'same'}]}\n"
     ]
    }
   ],
   "source": [
    "# Show metadata fields\n",
    "with open(\"test_specs/low/borb_line_length_comparison_meta.json\", \"r\") as f:\n",
    "    task_payload = json.load(f)\n",
    "\n",
    "print(task_payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "openai-demo",
   "metadata": {},
   "source": [
    "## Pipeline Demo with OpenAI API\n",
    "\n",
    "The following will walk you through the process of running and evaluating a single test using OpenAI API.\n",
    "\n",
    "The pipeline involves the following core components:\n",
    "\n",
    "1. **TaskLoader**: Loads the task data from based on the task metadata in the test_specs folder. It also contains the function used to encode the stimuli images.\n",
    "2. **OpenAIModelRunner**: This is a wrapper around the OpenAI API and handles the API calls. This wrapper ensures that the API settings and prompt format are consistent with that used in the original paper.  \n",
    "3. **Evaluator**: This file contains the Evaluator class that evaluates the model responses against the answer key stored on the task metadata in the test_specs folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c67c794",
   "metadata": {},
   "source": [
    "### Loading a task\n",
    "\n",
    "TaskLoader can be initialized to load a task information from a json file.\n",
    "\n",
    "For instance:\n",
    "\n",
    "```python\n",
    "loader = TaskLoader(\"test_specs/low/borb_line_length_comparison_meta.json\")\n",
    "```\n",
    "\n",
    "The attribute `loader.task_data` contains all the data stored in the json file.\n",
    "\n",
    "```python\n",
    "loader.task_data\n",
    "```\n",
    "\n",
    "The function `loader.get_task_info()` returns a dictionary containing the task information.\n",
    "\n",
    "```python\n",
    "loader.get_task_info()\n",
    "```\n",
    "\n",
    "While `loader.get_trials()` returns a list of dictionaries containing the trial-by-trial information.\n",
    "\n",
    "```python\n",
    "loader.get_trials()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a933c931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will run task: test_specs/low/borb_orientation_meta.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'stage': 'low',\n",
       " 'process': 'simple_element_judgements',\n",
       " 'task': 'borb_orientation',\n",
       " 'task_type': 'yes_no',\n",
       " 'num_stim': 'one',\n",
       " 'prompt': \"This is a task. In this task you will tell me if these two lines are parallel or not. Answer with only 'yes' or 'no'. Please put your final answer in {}.\",\n",
       " 'trials': [{'trial': 'trial_001',\n",
       "   'images': {'target': ['datasets/low/borb_orientation/001.png']},\n",
       "   'answer_key': 'yes'},\n",
       "  {'trial': 'trial_002',\n",
       "   'images': {'target': ['datasets/low/borb_orientation/002.png']},\n",
       "   'answer_key': 'no'},\n",
       "  {'trial': 'trial_003',\n",
       "   'images': {'target': ['datasets/low/borb_orientation/003.png']},\n",
       "   'answer_key': 'yes'},\n",
       "  {'trial': 'trial_004',\n",
       "   'images': {'target': ['datasets/low/borb_orientation/004.png']},\n",
       "   'answer_key': 'yes'},\n",
       "  {'trial': 'trial_005',\n",
       "   'images': {'target': ['datasets/low/borb_orientation/005.png']},\n",
       "   'answer_key': 'no'},\n",
       "  {'trial': 'trial_006',\n",
       "   'images': {'target': ['datasets/low/borb_orientation/006.png']},\n",
       "   'answer_key': 'yes'},\n",
       "  {'trial': 'trial_007',\n",
       "   'images': {'target': ['datasets/low/borb_orientation/007.png']},\n",
       "   'answer_key': 'yes'},\n",
       "  {'trial': 'trial_008',\n",
       "   'images': {'target': ['datasets/low/borb_orientation/008.png']},\n",
       "   'answer_key': 'no'},\n",
       "  {'trial': 'trial_009',\n",
       "   'images': {'target': ['datasets/low/borb_orientation/009.png']},\n",
       "   'answer_key': 'no'},\n",
       "  {'trial': 'trial_010',\n",
       "   'images': {'target': ['datasets/low/borb_orientation/010.png']},\n",
       "   'answer_key': 'yes'},\n",
       "  {'trial': 'trial_011',\n",
       "   'images': {'target': ['datasets/low/borb_orientation/011.png']},\n",
       "   'answer_key': 'no'},\n",
       "  {'trial': 'trial_012',\n",
       "   'images': {'target': ['datasets/low/borb_orientation/012.png']},\n",
       "   'answer_key': 'yes'},\n",
       "  {'trial': 'trial_013',\n",
       "   'images': {'target': ['datasets/low/borb_orientation/013.png']},\n",
       "   'answer_key': 'yes'},\n",
       "  {'trial': 'trial_014',\n",
       "   'images': {'target': ['datasets/low/borb_orientation/014.png']},\n",
       "   'answer_key': 'yes'},\n",
       "  {'trial': 'trial_015',\n",
       "   'images': {'target': ['datasets/low/borb_orientation/015.png']},\n",
       "   'answer_key': 'no'},\n",
       "  {'trial': 'trial_016',\n",
       "   'images': {'target': ['datasets/low/borb_orientation/016.png']},\n",
       "   'answer_key': 'no'},\n",
       "  {'trial': 'trial_017',\n",
       "   'images': {'target': ['datasets/low/borb_orientation/017.png']},\n",
       "   'answer_key': 'yes'},\n",
       "  {'trial': 'trial_018',\n",
       "   'images': {'target': ['datasets/low/borb_orientation/018.png']},\n",
       "   'answer_key': 'yes'},\n",
       "  {'trial': 'trial_019',\n",
       "   'images': {'target': ['datasets/low/borb_orientation/019.png']},\n",
       "   'answer_key': 'yes'},\n",
       "  {'trial': 'trial_020',\n",
       "   'images': {'target': ['datasets/low/borb_orientation/020.png']},\n",
       "   'answer_key': 'yes'},\n",
       "  {'trial': 'trial_021',\n",
       "   'images': {'target': ['datasets/low/borb_orientation/021.png']},\n",
       "   'answer_key': 'no'},\n",
       "  {'trial': 'trial_022',\n",
       "   'images': {'target': ['datasets/low/borb_orientation/022.png']},\n",
       "   'answer_key': 'no'},\n",
       "  {'trial': 'trial_023',\n",
       "   'images': {'target': ['datasets/low/borb_orientation/023.png']},\n",
       "   'answer_key': 'no'},\n",
       "  {'trial': 'trial_024',\n",
       "   'images': {'target': ['datasets/low/borb_orientation/024.png']},\n",
       "   'answer_key': 'yes'},\n",
       "  {'trial': 'trial_025',\n",
       "   'images': {'target': ['datasets/low/borb_orientation/025.png']},\n",
       "   'answer_key': 'no'},\n",
       "  {'trial': 'trial_026',\n",
       "   'images': {'target': ['datasets/low/borb_orientation/026.png']},\n",
       "   'answer_key': 'no'},\n",
       "  {'trial': 'trial_027',\n",
       "   'images': {'target': ['datasets/low/borb_orientation/027.png']},\n",
       "   'answer_key': 'no'},\n",
       "  {'trial': 'trial_028',\n",
       "   'images': {'target': ['datasets/low/borb_orientation/028.png']},\n",
       "   'answer_key': 'no'},\n",
       "  {'trial': 'trial_029',\n",
       "   'images': {'target': ['datasets/low/borb_orientation/029.png']},\n",
       "   'answer_key': 'yes'},\n",
       "  {'trial': 'trial_030',\n",
       "   'images': {'target': ['datasets/low/borb_orientation/030.png']},\n",
       "   'answer_key': 'no'}]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load one the test\n",
    "task_index = 0 \n",
    "print(f\"We will run task: {all_task_paths[task_index]}\")\n",
    "\n",
    "## LOAD TASK\n",
    "# The task loader requires a path to the task metadata\n",
    "# For example: `test_specs/low/borb_orientation_meta.json`\n",
    "loader = TaskLoader(all_task_paths[task_index])\n",
    "\n",
    "# Inspect task data stored in the loader\n",
    "loader.task_data # This attributes contain all the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0203e6",
   "metadata": {},
   "source": [
    "### Initialize the Model Runner and Run the Test\n",
    "\n",
    "ModelConfig is a dataclass that stores the configuration for the model runner. It contains the model name, max tokens, temperature, api key, and any additional parameters. This is used to initialize the model runner.\n",
    "\n",
    "We have three model runners already available: OpenAIModelRunner, AnthropicModelRunner, and GoogleModelRunner. These are the providers of models used in the original paper. If you want to use a different model, you can create a new model runner by inheriting from the ModelRunner class.\n",
    "\n",
    "Once the model runner is initialized, we can use it to run the tests. The model runner's function generate_response() takes a TaskLoader object as input and returns a tuple containing the task information and the model responses. This consists of the task information originally existing in the test metadata, but now with the addition of the model's responses for each trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "openai-single-task",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting model responses: 100%|██████████| 30/30 [00:29<00:00,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: borb_orientation\n",
      "Stage: low\n",
      "Process: simple_element_judgements\n",
      "Number of trials: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## INITIALIZE\n",
    "# Configure OpenAI model\n",
    "openai_config = ModelConfig(\n",
    "    model_name=\"gpt-4o-2024-05-13\", # model name\n",
    "    api_key=api_keys[\"openai\"], # api key # ensure to set this in utils/api_keys.json\n",
    "    max_tokens=1000, # max tokens used to generate the response\n",
    "    temperature=1.0 # default temperature for the model- also used in the original paper\n",
    ")\n",
    "\n",
    "# Initialize runner\n",
    "runner = OpenAIModelRunner(openai_config)\n",
    "\n",
    "## RUN THE TEST\n",
    "# Run the task\n",
    "results = runner.generate_response(loader) # return task info and results (payload + model responses)\n",
    "\n",
    "# Here `results` is a tuple containing the task info and the results\n",
    "print(f\"Task: {results[0]['task']}\")\n",
    "print(f\"Stage: {results[0]['stage']}\") # stage of the task: low, mid, or high\n",
    "print(f\"Process: {results[0]['process']}\") # which finer-grained process the task belongs to\n",
    "print(f\"Number of trials: {len(results[1])}\") # number of trials in the task\n",
    "\n",
    "# Print example model response\n",
    "print(results[1][0]['model_response']) #[1] returns trial-by-trial data, [0] reflects the first trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a9d049db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Model Responses:\n",
      "==================================================\n",
      "\n",
      "Trial trial_001:\n",
      "Prompt: This is a task. In this task you will tell me if these two lines are parallel or not. Answer with on...\n",
      "Model Response: {no}\n",
      "Correct Answer: yes\n",
      "------------------------------\n",
      "\n",
      "Trial trial_002:\n",
      "Prompt: This is a task. In this task you will tell me if these two lines are parallel or not. Answer with on...\n",
      "Model Response: {no}\n",
      "Correct Answer: no\n",
      "------------------------------\n",
      "\n",
      "Trial trial_003:\n",
      "Prompt: This is a task. In this task you will tell me if these two lines are parallel or not. Answer with on...\n",
      "Model Response: {no}\n",
      "Correct Answer: yes\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Show example of trial-by-trial details\n",
    "\n",
    "# Show first few responses\n",
    "print(\"Sample Model Responses:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Look at the results\n",
    "for i, trial in enumerate(results[1][:3]):\n",
    "    print(f\"\\nTrial {trial['trial_id']}:\")\n",
    "    print(f\"Prompt: {trial['prompt'][:100]}...\") # Prompts are same for all the trials within a task.\n",
    "    print(f\"Model Response: {trial['model_response']}\") # Final model responses are those indicated within {}\n",
    "    print(f\"Correct Answer: {trial['answer_key']}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a23f54f",
   "metadata": {},
   "source": [
    "### Evaluate the Results\n",
    "\n",
    "The current repository provides a class that helps automatically mark the model responses based on the criteria used in the paper.\n",
    "\n",
    "The evaluator takes the output tuple of the runner that contains both the overall task information and trial-by-trial information. The evaluator reads evaluation config file from `utils/evaluator_config.json`* to determine the method of evaluation for each task.\n",
    "\n",
    "Inputting the evaluator with the output will initialize marking process. The results are stored as a table and can be accessed using `.get_results()`.\n",
    "\n",
    "The results table can be saved as a CSV file using `.save_as_csv()`. Path can be specified as an argument. If no path is specified, the results will be saved as `results.csv` in the current directory.\n",
    "\n",
    "For every task evaluated the evaluator will store the corresponding result as a row in the table. Running and evaluating through all tasks will result in a table with the results for all tasks. *See below for how to run a batch of tasks*.\n",
    "\n",
    "<br>\n",
    "\n",
    "*`evaluator_config.json` is a file that contains the list of tasks that use each evaluation method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "evaluate-results",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>task_type</th>\n",
       "      <th>stage</th>\n",
       "      <th>process</th>\n",
       "      <th>num_trials</th>\n",
       "      <th>raw_score</th>\n",
       "      <th>percent_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>borb_orientation</td>\n",
       "      <td>yes_no</td>\n",
       "      <td>low</td>\n",
       "      <td>simple_element_judgements</td>\n",
       "      <td>30</td>\n",
       "      <td>17</td>\n",
       "      <td>0.566667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               task task_type stage                    process  num_trials  \\\n",
       "0  borb_orientation    yes_no   low  simple_element_judgements          30   \n",
       "\n",
       "   raw_score  percent_score  \n",
       "0         17       0.566667  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate results\n",
    "evaluator = Evaluator()\n",
    "evaluator.evaluate(results)\n",
    "\n",
    "# Display results\n",
    "results_df = evaluator.get_result() # get pandas dataframe\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2904331a",
   "metadata": {},
   "source": [
    "### Running a Batch of Tasks\n",
    "\n",
    "We can run loop through multiple tasks and input the results into the evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "batch-processing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing task 1/31: test_specs/low/borb_orientation_meta.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting model responses: 100%|██████████| 30/30 [00:28<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Completed: borb_orientation\n",
      "\n",
      "Processing task 2/31: test_specs/low/borb_line_length_comparison_meta.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting model responses: 100%|██████████| 30/30 [00:30<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Completed: borb_line_length_comparison\n",
      "\n",
      "Processing task 3/31: test_specs/low/borb_size_comparison_meta.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting model responses: 100%|██████████| 30/30 [00:30<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Completed: borb_size_comparison\n",
      "\n",
      "Processing task 4/31: test_specs/low/borb_position_of_gap_meta.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting model responses: 100%|██████████| 40/40 [00:44<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Completed: borb_position_of_gap\n",
      "\n",
      "Processing task 5/31: test_specs/low/mindset_weber_law_meta.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting model responses: 100%|██████████| 40/40 [00:40<00:00,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Completed: mindset_weber_law\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run a subset of tasks\n",
    "n_tasks = 5\n",
    "\n",
    "# Initialize evaluator\n",
    "batch_evaluator = Evaluator()\n",
    "batch_results = []\n",
    "\n",
    "# Loop through tasks in our list of task paths\n",
    "# For each task\n",
    "for i, task_path in enumerate(all_task_paths[:n_tasks]): \n",
    "    print(f\"\\nProcessing task {i+1}/{len(all_task_paths)}: {task_path}\")\n",
    "    \n",
    "    loader = TaskLoader(task_path) # Load the task data\n",
    "    runner = OpenAIModelRunner(openai_config) # Initialize the runner\n",
    "    results = runner.generate_response(loader) # Get the model response\n",
    "    batch_evaluator.evaluate(results) # Evaluate the result, this will append to the result table stored \n",
    "    print(f\"✓ Completed: {results[0]['task']}\")\n",
    "\n",
    "# Save results\n",
    "batch_evaluator.save_as_csv(f\"results_{openai_config.model_name}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minibench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
