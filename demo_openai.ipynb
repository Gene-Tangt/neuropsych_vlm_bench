{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import os\n",
    "from evaluator import Evaluator\n",
    "from runner import ModelConfig, OpenAIModelRunner\n",
    "from loaders import TaskLoader\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb0ac83",
   "metadata": {},
   "source": [
    "## Load API keys and task paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get API keys\n",
    "with open(\"utils/api_keys.json\", \"r\") as f:\n",
    "    api_keys = json.load(f)\n",
    "\n",
    "# Load task paths\n",
    "with open(\"test_specs/test_list.json\", 'r') as file:\n",
    "    all_task_paths = []\n",
    "    for stage in json.load(file):\n",
    "        all_task_paths.extend(stage['task_paths'])\n",
    "\n",
    "print(f\"Found {len(all_task_paths)} tasks\")\n",
    "print(\"Sample tasks:\")\n",
    "for i, path in enumerate(all_task_paths[:3]): # print first 3 tasks\n",
    "    print(f\"  {i}: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "openai-demo",
   "metadata": {},
   "source": [
    "## Demo with OpenAI API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6224d73",
   "metadata": {},
   "source": [
    "### Running a single task ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "openai-single-task",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure OpenAI model\n",
    "openai_config = ModelConfig(\n",
    "    model_name=\"gpt-4o-2024-05-13\",\n",
    "    api_key=api_keys[\"open_ai\"],\n",
    "    max_tokens=1000,\n",
    "    temperature=1.0\n",
    ")\n",
    "\n",
    "# Create runner using factory function\n",
    "runner = OpenAIModelRunner(openai_config)\n",
    "\n",
    "# Run a single task\n",
    "task_index = 12 \n",
    "print(f\"Running task: {all_task_paths[task_index]}\")\n",
    "\n",
    "loader = TaskLoader(all_task_paths[task_index]) # Use TaskLoader to load the task\n",
    "results = runner.generate_response(loader) # return task info and results (payload + model responses)\n",
    "\n",
    "print(f\"Task: {results[0]['task']}\")\n",
    "print(f\"Stage: {results[0]['stage']}\") # stage of the task: low, mid, or high\n",
    "print(f\"Process: {results[0]['process']}\") # which finer-grained process the task belongs to\n",
    "print(f\"Number of trials: {len(results[1])}\") # number of trials in the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate results\n",
    "evaluator = Evaluator()\n",
    "evaluator.evaluate(results)\n",
    "\n",
    "# Display results\n",
    "results_df = evaluator.get_result() # get pandas dataframe\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect-responses",
   "metadata": {},
   "source": [
    "## Inspect Payloads\n",
    "Each task payload contains `task_info` and `results`\n",
    "Task info is a dictionary containing the task information.\n",
    "Results is a list of dictionaries containing the trial information (e.g., prompt, conversation content, answer key, and model response for each trial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-responses",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first few responses\n",
    "print(\"Sample Model Responses:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Look at the results\n",
    "for i, trial in enumerate(results[1][:3]):\n",
    "    print(f\"\\nTrial {trial['trial_id']}:\")\n",
    "    print(f\"Prompt: {trial['prompt'][:100]}...\") # Prompts are same for all the trials within a task.\n",
    "    print(f\"Model Response: {trial['model_response']}\") # Final model responses are those indicated within {}\n",
    "    print(f\"Correct Answer: {trial['answer_key']}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "batch-demo",
   "metadata": {},
   "source": [
    "## Batch Processing Demo\n",
    "Run multiple tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "batch-processing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run multiple tasks\n",
    "batch_evaluator = Evaluator()\n",
    "batch_results = []\n",
    "\n",
    "for i, task_path in enumerate(all_task_paths): \n",
    "    print(f\"\\nProcessing task {i+1}/{len(all_task_paths)}: {task_path}\")\n",
    "    \n",
    "    loader = TaskLoader(task_path)\n",
    "    runner = OpenAIModelRunner(openai_config)\n",
    "    results = runner.generate_response(loader)\n",
    "    batch_evaluator.evaluate(results)\n",
    "    print(f\"âœ“ Completed: {results[0]['task']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "batch-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show batch results\n",
    "batch_evaluator.get_result() # get_results appends all the results evaluated after Evaluator initialization\n",
    "batch_evaluator.save_as_csv(f\"results_{openai_config.model_name}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minibench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
